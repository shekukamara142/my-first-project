{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "e5NZQ_7vrG4G"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03BLxlsarK8E"
   },
   "source": [
    "# solution 1 Creating a one-dimensional convolutional layer class that limits the number of channels to one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qi3wQEeVrVxS"
   },
   "source": [
    "Create a 1D convolutional layer class SimpleConv1d with the number of channels limited to 1. The basic structure will be the same as that of the FC class for the full convolutional layer created in the previous Sprint. The initial values of Xavier are the same as for the full convolution layer.\n",
    "\n",
    "Padding is not considered here, and the stride is fixed to 1. You don't need to think about processing multiple data at the same time, and the batch size should be 1 only. The extension of this part will be an advanced topic.\n",
    "\n",
    "The formula for forward propagation is as follows.\n",
    "$$\\alpha_i = \\sum_{s=0}^{F-1}x_{(i+s)}W_s + b$$.\n",
    "\n",
    "$a_i$ : i-th value of the output array\n",
    "\n",
    "$F$ : Size of the filter\n",
    "\n",
    "$x_{(i+s})$ : (i+s)th value of the input array\n",
    "\n",
    "$w_s$ : s-th value of the array of weights\n",
    "\n",
    "$b$ : Bias term\n",
    "\n",
    "All are scalars.\n",
    "\n",
    "Next is the update formula. This is the same as the full coupling layer in that it can be replaced by AdaGrad, etc.\n",
    "$$w'_s = w_s -\\alpha\\frac{\\partial L}{\\partial w_s}$$$$b' = b -\\alpha\\frac{\\partial L}{\\partial b}$$\n",
    "\n",
    "$\\alpha$ : Learning rate\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_s}$ : Gradient of loss $L$ for w_s\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b}$ : Gradient of loss $L$ for $b$.\n",
    "\n",
    "Here are the backpropagation formulas for finding the gradient $\\frac{\\partial L}{\\partial w_s}$ and $\\frac{\\partial L}{\\partial b}$.\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_s} = \\sum_{i=0}^{N_{out}-1}\\frac{\\partial L}{\\partial a_i}x_{(i+s}$$$$\\frac{\\partial L}{\\partial b} = \\sum_{i=0}^{N_{out}-1}\\frac{\\partial L}{\\partial a_i}$$\n",
    "$\\frac{\\partial L}{\\partial a_i}$: i-th value of the gradient array\n",
    "\n",
    "$N_{out} : Size of the output\n",
    "\n",
    "The formula for the error to be passed to the previous layer is as follows.　　 $$\\frac{\\partial L}{\\partial x_j} = \\sum_{s=0}^{F-1}\\frac{\\partial L}{\\partial a_{j-s}}w_s$$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x_j}$ : j-th value of the array of errors to be passed to the previous layer.\n",
    "\n",
    "However, when $j-s&lt;0$ or $j-s&gt;N_{out}-1$, $\\frac{\\partial L}{\\partial a_{j-s}}=0$.\n",
    "\n",
    "The main difference from the full join layer is that the weights are shared for multiple features. In this case, the gradient is obtained by adding all the errors of the shared ones. The branching in the computational graph can be done by adding up the errors during backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fl8gR_aGt8rG"
   },
   "source": [
    "#  Output size calculation after one-dimensional convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApASeBUgt_ho"
   },
   "source": [
    "The number of features changes when convolution is performed. How it changes can be obtained from the following formula. Padding and stride are also included. Create a function to do this calculation.\n",
    "$$N_{out} = \\frac{N_{in} + 2P - F}{S} + 1$$.\n",
    "\n",
    "$$N_{out}$$ : Size of the output (number of features)\n",
    "\n",
    "$N_{in}$ : size of the input (number of features)\n",
    "\n",
    "$P$ : Number of paddings in a direction\n",
    "\n",
    "$F$ : Size of the filter\n",
    "\n",
    "$S$ : Size of stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YqzRvYavuH6B"
   },
   "outputs": [],
   "source": [
    "# Sample data\n",
    "x = np.array([1,2,3,4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])\n",
    "delta_a = np.array([10, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hXiPs2EguReM"
   },
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    All coupling layers from n_nodes1 to n_nodes2\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      Number of nodes in previous layer\n",
    "    n_nodes2 : int\n",
    "      Number of nodes in the next layer\n",
    "    initializer : instance of initialization method\n",
    "    optimizer : instance of optimisation method\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        # Initialization\n",
    "        # Use the initializer method to initialize self.W and self.B\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.optimizer = optimizer\n",
    "        #self.W = initializer.W(self.n_nodes1, self.n_nodes2)\n",
    "        #self.B = initializer.B(self.n_nodes2)\n",
    "        self.W = np.array([3,5,7]) \n",
    "        self.B = np.array([1])\n",
    "        self.P = 0\n",
    "        self.Str = 1\n",
    "        self.s = len(self.W)\n",
    "        self.a = np.array([])\n",
    "        self.dW = np.array([])\n",
    "        self.dX = np.array([])\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of the following form, shape (batch_size, n_nodes_bf)\n",
    "            Input\n",
    "        Returns\n",
    "        ----------\n",
    "        A : ndarray of the following form, shape (batch_size, n_nodes_af)\n",
    "            Outputs\n",
    "        \"\"\"\n",
    "        #self.X = X\n",
    "        self.X = X\n",
    "        self.Xsize = len(self.X)\n",
    "        self._output_size()\n",
    "        \n",
    "        self.a = np.append(self.a, np.array([(self.X[i:i+self.s] @ self.W.T + self.B) for i in range(self.Nout)]))\n",
    "        return self.a\n",
    "    \n",
    "    def _output_size(self):\n",
    "        self.Nout = int((len(self.X) + 2*self.P - self.s) / self.Str + 1)       \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backward\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : ndarray of the following form, shape (batch_size, n_nodes2)\n",
    "            The gradient flowed from behind\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : ndarray of the next shape, shape (batch_size, n_nodes1)\n",
    "            Gradient flowing forward\n",
    "        \"\"\"\n",
    "        self.dB = np.sum(dA, axis=0)\n",
    "        self.dW = np.append(self.dW, np.array([(dA @ self.X[i:i+self.Nout].T) for i in range(self.s)]))\n",
    "\n",
    "        for j in range(len(self.X)):\n",
    "            if j-self.Nout < 0:\n",
    "                #display(j)\n",
    "                #display(np.flip(self.W[ : self.s-(self.Nout-j) ]))\n",
    "                #display(dA[:j+1])\n",
    "                self.dX = np.append(self.dX, (dA[:j+1] @ np.flip(self.W[ : self.s-(self.Nout-j) ]).T))\n",
    "                \n",
    "            elif j > len(self.X) - self.Nout:\n",
    "                #display(j)\n",
    "                #display(np.flip(dA    [-(j-(self.Xsize - self.Nout)):]))\n",
    "                #display(np.flip(self.W[-(j-(self.Xsize - self.Nout)):]))\n",
    "                self.dX = np.append(self.dX, (np.flip(dA    [-(j-(self.Xsize - self.Nout)):])) @ \n",
    "                                              np.flip(self.W[-(j-(self.Xsize - self.Nout)):]).T)\n",
    "            else:\n",
    "                #display(j)\n",
    "                #display(np.flip(self.W[ j-self.Nout+1 : j+1 ]))\n",
    "                self.dX = np.append(self.dX, (dA @ np.flip(self.W[ j-self.Nout+1 : j+1 ]).T))\n",
    "\n",
    "        # Updates\n",
    "        #self.W = self.optimizer.update(dW, self.W)\n",
    "        #self.B = self.optimizer.update(dB, self.B)\n",
    "\n",
    "        return self.dX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGeVtvnLwcC6"
   },
   "source": [
    "#  Experiment of one-dimensional convolutional layer with small array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5eLGKaOwd_o"
   },
   "source": [
    "Check that the forward and back propagation is correct in the small array shown below.\n",
    "\n",
    "Let the input $x$, the weight $w$ and the bias $b$ be as follows\n",
    "\n",
    "x = np.array([1,2,3,4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])\n",
    "With forward propagation, the output looks like this\n",
    "\n",
    "a = np.array([35, 50])\n",
    "Now consider backpropagation. Suppose the error is as follows\n",
    "\n",
    "delta_a = np.array([10, 20])\n",
    "If we backpropagate, we get the following value\n",
    "\n",
    "delta_b = np.array([30])\n",
    "delta_w = np.array([50, 80, 110])\n",
    "delta_x = np.array([30, 110, 170, 140])\n",
    "\n",
    "Implementation considerations\n",
    "To implement convolution, you can start with a series of for statements. However, we want to make the computation as efficient as possible, so we will consider a way to compute the following expression at once.\n",
    "$$a_i = \\sum_{s=0}^{F-1}x_{i+s}w_s + b$$.\n",
    "\n",
    "The bias term is a simple addition, so we look at the weight part. $$\\sum_{s=0}^{F-1}x_{i+s}w_s$$.\n",
    "\n",
    "This is the inner product of an array of w with a portion of x taken out of it. Given a concrete situation, we can calculate it with the following code. In this example, in order to make the flow easier to understand, the adamant product is calculated between each element and then the sum is calculated. This results in the same as the inner product.\n",
    "\n",
    "x = np.array([1, 2, 3, 4])\n",
    "w = np.array([3, 5, 7])\n",
    "\n",
    "a = np.empty((2, 3))\n",
    "\n",
    "indexes0 = np.array([0, 1, 2]).astype(np.int)\n",
    "indexes1 = np.array([1, 2, 3]).astype(np.int)\n",
    "\n",
    "a[0] = x[indexes0]w # x[indexes0]は([1, 2, 3])である a[1] = x[indexes1]w # x[indexes1]is ([2, 3, 4])\n",
    "\n",
    "a = a.sum(axis=1)\n",
    "The ndarray method makes use of the fact that arrays can be indexed.\n",
    "\n",
    "You can also use a two-dimensional array to get a two-dimensional array from a one-dimensional array.\n",
    "\n",
    "x = np.array([1, 2, 3, 4])\n",
    "indexes = np.array([[0, 1, 2], [1, 2, 3]]).astype(np.int)\n",
    "\n",
    "print(x[indexes]) # ([[1, 2, 3], [2, 3, 4]])\n",
    "With a good combination of this and a broadcast or similar, it is possible to compute them all at once.\n",
    "\n",
    "There is no right answer to the calculation method of convolution, so please make it efficient in your own way.\n",
    "\n",
    "Reference\n",
    "The part of Integer array indexing in the following page is a description of this method.\n",
    "\n",
    "Indexing - NumPy v1.17 Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "jyQnlkGJxcZc"
   },
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])\n",
    "test = FC(1,2,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y1pJ6572xg-1",
    "outputId": "185ac283-c545-4747-c36a-4651177334fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35., 50.])"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JU1NpUF1xjch",
    "outputId": "74ec04ea-8d4d-4cd4-abf8-085dca8e030e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 30., 110., 170., 140.])"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_a = np.array([10, 20])\n",
    "test.backward(delta_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "ggIG43qRxmxE",
    "outputId": "00cf39f4-db61-4961-d1a2-26ac17c9206c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 50.,  80., 110.])"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(test.dB)\n",
    "display(test.dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 135
    },
    "id": "TO6kpd75xqKc",
    "outputId": "93ec1d50-6b89-44ac-8a85-6845c960e0ae"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-721273b29d20>\"\u001b[0;36m, line \u001b[0;32m43\u001b[0m\n\u001b[0;31m    self.X = X\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "class Conv1d:\n",
    "    \"\"\"\n",
    "    All join layers from number of nodes n_nodes1 to n_nodes2\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      Number of nodes in previous layer\n",
    "    n_nodes2 : int\n",
    "      Number of nodes in the next layer\n",
    "    initializer : instance of initialization method\n",
    "    optimizer : instance of the optimisation method\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        # Initialization.\n",
    "        # Use the methods of initializer to initialize self.W and self.B\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.optimizer = optimizer\n",
    "        #self.W = initializer.W(self.n_nodes1, self.n_nodes2)\n",
    "        #self.B = initializer.B(self.n_nodes2)\n",
    "        self.W = np.array([3,5,7]) \n",
    "        self.B = np.array([1])\n",
    "        self.P = 0\n",
    "        self.Str = 1\n",
    "        self.s = len(self.W)\n",
    "        self.a = np.array([])\n",
    "        self.dW = np.array([])\n",
    "        self.dX = np.array([])\n",
    "\n",
    "    def forward(self, X):\n",
    "         \"\"\"\n",
    "        Forward\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of the following form, shape (batch_size, n_nodes_bf)\n",
    "            Input\n",
    "        Returns\n",
    "        ----------\n",
    "        A : ndarray of the following form, shape (batch_size, n_nodes_af)\n",
    "            Outputs\n",
    "        \"\"\"\n",
    "        #self.X = X\n",
    "        self.X = X\n",
    "        self.Xsize = len(self.X)\n",
    "        self._output_size()\n",
    "        \n",
    "        self.a = np.append(self.a, np.array([(self.X[i:i+self.s] @ self.W.T + self.B) for i in range(self.Nout)]))\n",
    "        return self.a\n",
    "    \n",
    "    def _output_size(self):\n",
    "        self.Nout = int((len(self.X) + 2*self.P - self.s) / self.Str + 1)       \n",
    "            \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backwards\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : ndarray of the following form, shape (batch_size, n_nodes2)\n",
    "            Gradient flowing from behind\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : ndarray of the following form, shape (batch_size, n_nodes1)\n",
    "            Forward flow gradient\n",
    "        \"\"\"\n",
    "        self.dB = np.sum(dA, axis=0)\n",
    "        self.dW = np.append(self.dW, np.array([(dA @ self.X[i:i+self.Nout].T) for i in range(self.s)]))\n",
    "\n",
    "        for j in range(len(self.X)):\n",
    "            if j-self.Nout < 0:\n",
    "                #display(j)\n",
    "                #display(np.flip(self.W[ : self.s-(self.Nout-j) ]))\n",
    "                #display(dA[:j+1])\n",
    "                self.dX = np.append(self.dX, (dA[:j+1] @ np.flip(self.W[ : self.s-(self.Nout-j) ]).T))\n",
    "                \n",
    "            elif j > len(self.X) - self.Nout:\n",
    "                #display(j)\n",
    "                #display(np.flip(dA    [-(j-(self.Xsize - self.Nout)):]))\n",
    "                #display(np.flip(self.W[-(j-(self.Xsize - self.Nout)):]))\n",
    "                self.dX = np.append(self.dX, (np.flip(dA    [-(j-(self.Xsize - self.Nout)):])) @ \n",
    "                                              np.flip(self.W[-(j-(self.Xsize - self.Nout)):]).T)\n",
    "            else:\n",
    "                #display(j)\n",
    "                #display(np.flip(self.W[ j-self.Nout+1 : j+1 ]))\n",
    "                self.dX = np.append(self.dX, (dA @ np.flip(self.W[ j-self.Nout+1 : j+1 ]).T))\n",
    "                \n",
    "        # Updates\n",
    "        #self.W = self.optimizer.update(dW, self.W)\n",
    "        #self.B = self.optimizer.update(dB, self.B)\n",
    "\n",
    "        return self.dX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ji0h-lVLzgzM"
   },
   "source": [
    "#  Creating a one-dimensional convolutional layer class that does not limit the number of channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zTWkJysAmr7"
   },
   "source": [
    "Create a class Conv1d for a 1D convolutional layer that does not limit the number of channels to 1.\n",
    "\n",
    "For example, if you have x, w, b as follows\n",
    "\n",
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape(2, 4), with (number of input channels, number of features).\n",
    "w = np.ones((3, 2, 3)) # All set to 1 for simplicity of the example. (number of output channels, number of input channels, filter size).\n",
    "b = np.array([1, 2, 3]) # (number of output channels)\n",
    "The output will look like this\n",
    "\n",
    "a = np.array([[16, 22], [17, 23], [18, 24]]) # shape(3, 2), which is (number of output channels, number of features).\n",
    "This is an example with 2 input channels and 3 output channels. After drawing the computational graph, let's also consider backpropagation by hand calculation. Since only sums and products appear in the computational graph, there is no need to think about differentiation anew.\n",
    "\n",
    "Supplemental\n",
    "When adding the number of channels, there is the problem of what order to put the arrays in. The most common order is (batch size, number of channels, number of features) or (batch size, number of features, number of channels), and the order varies depending on the library. The order differs depending on the library, and some can be switched.\n",
    "\n",
    "In this example, the batch size is the number of channels. In the above example, the batch size is not considered, but (number of channels, number of features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PgjvcIu9A0yE"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zPZxSBm7A79u"
   },
   "outputs": [],
   "source": [
    "class Conv1d:\n",
    "    \"\"\"\n",
    "    All coupling layers from n_nodes1 to n_nodes2\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      Number of nodes in previous layer\n",
    "    n_nodes2 : int\n",
    "      Number of nodes in the next layer\n",
    "    initializer : instance of initialization method\n",
    "    optimizer : instance of optimization method\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.P = 0\n",
    "        self.Str = 1\n",
    "        self.a = np.array([])\n",
    "        self.dW = np.array([])\n",
    "        self.dX = np.array([])\n",
    "        #self.s=None\n",
    "    def forward(self, X, W ,B):\n",
    "        \"\"\"\n",
    "        Forward\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of the following form, shape (batch_size, n_nodes_bf)\n",
    "            Input\n",
    "        Returns\n",
    "        ----------\n",
    "        A : ndarray of the following form, shape (batch_size, n_nodes_af)\n",
    "            Outputs\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.Xsize = self.X.shape[1]\n",
    "        self.W = W \n",
    "        self.B = B\n",
    "        self.s = self.W.shape[2]\n",
    "        self._output_size()\n",
    "        self.a = np.append(self.a, np.array([np.sum(self.X[:, i:i+self.s] * self.W[i0,:,:]) + self.B[i0] \n",
    "                                             for i0,i in itertools.product(range(self.W.shape[0]), range(self.Nout))]))\n",
    "        self.a = self.a.reshape(self.W.shape[0], self.Nout)\n",
    "        return self.a\n",
    "    def _output_size(self):\n",
    "        self.Nout = int((self.Xsize + 2*self.P - self.s) / self.Str + 1)       \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backward\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : ndarray of the following form, shape (batch_size, n_nodes2)\n",
    "            Gradient flowed from behind\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : ndarray of the next shape, shape (batch_size, n_nodes1)\n",
    "            Gradient flowing forward\n",
    "        \"\"\"\n",
    "        self.dB = np.sum(dA, axis=1)   \n",
    "        for i0 in range(self.W.shape[0]):\n",
    "            dX_ary = np.zeros((self.W.shape[1], self.W.shape[2]))\n",
    "            for i in range(self.Nout):\n",
    "                dX_ary += dA[i0,i] * self.X[:, i:i+self.s]\n",
    "            self.dW = np.append(self.dW, dX_ary)\n",
    "        self.dW = self.dW.reshape(3,2,3)\n",
    "        \"\"\"\n",
    "        Under consideration\n",
    "        self.Xp=np.array([self.X]*self.W.shape[0])\n",
    "        self.\n",
    "        self.dW= np.zeros((self.W.shape))\n",
    "        self.dW = [self.dW + (dA * self.Xp[:,:,i:i+self.s]) for i in range(self.Nout)]\n",
    "        \"\"\"\n",
    "        #self.dW = np.append(self.dW, [dX_ary + (np.ones((self.W[1].shape, self.W[2].shape))*dA[i0,i]) * self.X[:, i:i+self.Nout] \n",
    "        #                              for i in range(self.Nout)] for i0 in range(self.W.shape[0]))\n",
    "        \n",
    "        #for j in range(len(self.X)):\n",
    "        #    if j-self.Nout < 0:\n",
    "        #        self.dX = np.append(self.dX, (dA[:j+1] @ np.flip(self.W[ : self.s-(self.Nout-j) ]).T))\n",
    "        #        \n",
    "        #    elif j > len(self.X) - self.Nout:\n",
    "        #        self.dX = np.append(self.dX, (np.flip(dA    [-(j-(self.Xsize - self.Nout)):])) @ \n",
    "        #                                      np.flip(self.W[-(j-(self.Xsize - self.Nout)):]).T)\n",
    "        #    else:\n",
    "        #        self.dX = np.append(self.dX, (dA @ np.flip(self.W[ j-self.Nout+1 : j+1 ]).T))\n",
    "\n",
    "        return self.dW\n",
    "    \n",
    "    #def _naiseki_plus(self,dA):\n",
    "    #    dX_ary = np.zeros((self.W[1].shape, self.W[2].shape))\n",
    "    #    dX_ary = [dX_ary + (np.ones((self.W[1].shape, self.W[2].shape))*dA[i0,i]) * self.X[:, i:i+self.Nout] for i in range(self.Nout)]\n",
    "    #    return dX_ary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IQWgQcLuCZYD"
   },
   "outputs": [],
   "source": [
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape(2, 4), (number of input channels, number of features).\n",
    "w = np.ones((3, 2, 3)) # all 1 for simplicity of the example. (number of output channels, number of input channels, filter size).\n",
    "b = np.array([1, 2, 3]) # (number of output channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-vQNU47CiR5"
   },
   "outputs": [],
   "source": [
    "test2dim = Conv1d()\n",
    "test2dim.forward(x,w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zE1yra2dCs5r"
   },
   "outputs": [],
   "source": [
    "x = np.array([[2, 3, 4, 5], [1, 2, 3, 4]]) # shape(2, 4), (number of input channels, number of features).\n",
    "#w = np.ones((3, 2, 3)) # all 1 for simplicity of the example. (number of output channels, number of input channels, filter size).\n",
    "w = np.array([[[1,1,1],\n",
    "            [1,1,1]],\n",
    "            [[1,1,1],\n",
    "            [2,1,1]],\n",
    "            [[2,1,1]],\n",
    "            [1,1,2]])\n",
    "b = np.array([3, 2, 1]) # (number of output channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y0_lO7ffCweW"
   },
   "outputs": [],
   "source": [
    "test2dim = Conv1d()\n",
    "test2dim.forward(x,w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vWuRawIgCy0X"
   },
   "outputs": [],
   "source": [
    "dA = np.array([[52,56],\n",
    "            [32,35],\n",
    "            [9,11]])\n",
    "#dA = np.array([9,11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c80xRT1JC1cu"
   },
   "outputs": [],
   "source": [
    "dA3=np.array([dA]*3)\n",
    "dA3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ER86dzyPC34C"
   },
   "outputs": [],
   "source": [
    "dA3=dA3.transpose(1,2,0)\n",
    "dA3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZGxfO3aMC6nt"
   },
   "outputs": [],
   "source": [
    "test2dim.backward(dA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SI0uv1vEDyf5"
   },
   "source": [
    "#  Learning and estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b23OKgaFD0Gq"
   },
   "source": [
    "Train and estimate MNIST by replacing some of the full coupling layers of the neural network you have been using with Conv1d and calculate Accuracy. Only the output layer should use the full coupling layer. However, if you have multiple channels, you cannot input to all coupled layer. At that stage, the channels should be set to 1 or they should be smoothed. Since 1D convolution of an image is not practical, accuracy is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehW2w-r0D7k4"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "(X_train, t_train), (X_test, t_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-n5i4DED-yb"
   },
   "outputs": [],
   "source": [
    "X_train  = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "enc = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "t_train_one_hot = enc.fit_transform(t_train[:, np.newaxis])\n",
    "t_test_one_hot = enc.fit_transform(t_test[:,  np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7NBLGTyMECzW"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000, 1, 784)\n",
    "X_test = X_test.reshape(-1, 1, 784)\n",
    "w = np.ones((3, 1, 3))\n",
    "b = np.array([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xkWGRhv9EgZ2"
   },
   "outputs": [],
   "source": [
    "class CNN_mnist_FC:\n",
    "    \"\"\"\n",
    "    Convolutional layer\n",
    "    Parameters\n",
    "    ----------\n",
    "    w:convolutional layer weights w.shape (output channel, input channel, filter size)\n",
    "    b:Bias of the convolutional layer b.shape (output channel, )\n",
    "    stride:Number of strides\n",
    "    padding:Number of paddings\n",
    "    optimizer : instance of the optimization method\n",
    "    \"\"\"\n",
    "    def __init__(self, w, b , optimizer,stride, padding):\n",
    "        self.optimizer = optimizer\n",
    "        self.W = w\n",
    "        self.B = b\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of the following form, shape (batch_size, n_nodes1)\n",
    "            Input\n",
    "        Returns\n",
    "        ----------\n",
    "        A : ndarray of the following form, shape (batch_size, n_nodes2)\n",
    "            Outputs\n",
    "        \"\"\"\n",
    "        self.A = X\n",
    "        output_size, chanel_size, filter_size = self.W.shape\n",
    "        feature_size = self.A.shape[2]\n",
    "        sample_size = self.A.shape[0]\n",
    "\n",
    "        a = np.zeros([sample_size, output_size, feature_size-2])\n",
    "        for samples in range(sample_size):\n",
    "            for output in range(output_size):\n",
    "                for j in range(filter_size - 1):\n",
    "                    sig = 0\n",
    "                    for chanel in range(chanel_size):\n",
    "                        for i in range(filter_size):\n",
    "                            sig += X[samples, chanel, i+j] * self.W[output, chanel, j]\n",
    "                    a[samples, output, j] = sig + b[output]\n",
    "        \n",
    "        return a\n",
    "\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        backward\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : ndarray of the following form, shape (batch_size, n_nodes2)\n",
    "            The gradient flowed from backward\n",
    "        Return value\n",
    "        ----------\n",
    "        dZ : ndarray of the following form, shape (batch_size, n_nodes1)\n",
    "            Gradient flowing forward\n",
    "        \"\"\"\n",
    "        self.n_out = N_OUT(self.stride, self.padding, self.W, self.A)\n",
    "        \n",
    "        output_size, chanel_size, filter_size = self.W.shape\n",
    "        feature_size = self.A.shape[2]\n",
    "        sample_size = self.A.shape[0]\n",
    "        \n",
    "        #LB calculation\n",
    "        self.LB = dA.sum(axis=0)\n",
    "        self.LB = self.LB.sum(axis=1)\n",
    "        \n",
    "        #Calculation of LW\n",
    "        self.LW = np.zeros_like(self.W)\n",
    "        for samples in range(sample_size):\n",
    "            for output in range(output_size):\n",
    "                for chanel in range(chanel_size):\n",
    "                    for i in range(filter_size):\n",
    "                        for j in range(filter_size -1):\n",
    "                            self.LW[output, chanel, i] += dA[samples, output, j]*self.A[samples, chanel, j+i]\n",
    "                        \n",
    "                        \n",
    "\n",
    "                    \n",
    "                    \n",
    "        Calculation of #dZ\n",
    "        dZ = np.zeros_like(self.A)\n",
    "        for samples in range(sample_size):\n",
    "            for output in range(output_size):\n",
    "                for chanel in range(chanel_size):\n",
    "                    for j in range(feature_size):\n",
    "                        sigma=0\n",
    "                        for s in range(filter_size):\n",
    "                            if j - s < 0 or j - s > self.n_out -1:\n",
    "                                pass\n",
    "                            else:\n",
    "                                sigma += dA[samples, output,  j-s] * self.W[output, chanel, s]\n",
    "                        dZ[samples, chanel, j] += sigma\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        # Updates\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OkHVIGxVF8Ma"
   },
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def forward(self, X):\n",
    "        self.A = X\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def backward(self, Z):\n",
    "        \n",
    "        return Z * np.maximum(np.sign(self.A), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emyED_aeGPgK"
   },
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    All join layers from n_nodes1 to n_nodes2\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      Number of nodes in previous layer\n",
    "    n_nodes2 : int\n",
    "      Number of nodes in the next layer\n",
    "    initializer : instance of initialization method\n",
    "    optimizer : instance of the optimisation method\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # Initialize.\n",
    "        # Use the methods of initializer to initialize self.W and self.B\n",
    "        init = initializer\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.W = init.W(n_nodes1, n_nodes2)\n",
    "        self.B = init.B(n_nodes2)\n",
    "    \n",
    "\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of the following form, shape (batch_size, n_nodes1)\n",
    "            Input\n",
    "        Returns\n",
    "        ----------\n",
    "        A : ndarray of the following form, shape (batch_size, n_nodes2)\n",
    "            Outputs\n",
    "        \"\"\"\n",
    "        self.z = X\n",
    "        self.a = X@self.W + self.B\n",
    "        \n",
    "        return self.a\n",
    "\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backward\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : ndarray of the following form, shape (batch_size, n_nodes2)\n",
    "            The gradient flowed from behind\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : ndarray of the next shape, shape (batch_size, n_nodes1)\n",
    "            Gradient flowing forward\n",
    "        \"\"\"\n",
    "        dZ = dA @ self.W.T\n",
    "        self.LW = self.z.T @ dA\n",
    "        self.LB = np.sum(dA, axis=0)\n",
    "        \n",
    "        \n",
    "        # Update\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4z3tFDNJGYMS"
   },
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B  = self.sigma * np.random.randn(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZMw96EndGb-v"
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self, A):\n",
    "        exp_a = np.exp(A)\n",
    "        softmax_result = np.empty((A.shape[0], A.shape[1]))\n",
    "        exp_sum = np.sum(exp_a, axis=1)\n",
    "        for i in range(A.shape[0]):\n",
    "            softmax_result[i] = exp_a[i] / exp_sum[i]\n",
    "            \n",
    "        return softmax_result\n",
    "    \n",
    "    def backward(self, Z, Y):\n",
    "        \n",
    "        L_A = Z - Y\n",
    "        self.cross_entropy = -np.average(np.sum(Y*np.log(Z), axis=1))\n",
    "        \n",
    "        \n",
    "        return L_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TkfskCltGmDX"
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    Stochastic gradient descent method\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : Learning rate\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Update the weights and bias of a layer\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : the instance of the layer before the update\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : the instance of the layer after the update\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        layer.W = layer.W - self.lr * layer.LW\n",
    "        \n",
    "        layer.B = layer.B - self.lr*layer.LB\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VS5NBDz4Gqr5"
   },
   "outputs": [],
   "source": [
    "def N_OUT(stride, padding, X,  W):\n",
    "    if X.ndim == 1:\n",
    "        return int((X.shape[0] + (2*padding) - len(W) / stride) + 1)\n",
    "    elif X.ndim == 3:\n",
    "        return int((X.shape[2] + (2*padding) - len(W) / stride) + 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yOywvLoRGuv3"
   },
   "outputs": [],
   "source": [
    "cnn_mnist = CNN_mnist_FC(w, b, SGD(0.1), 1, 0)\n",
    "A = cnn_mnist.forward(X_train)\n",
    "relu = Relu()\n",
    "A_relu = relu.forward(A)\n",
    "A_flat = A_relu.reshape(A_relu.shape[0], -1)\n",
    "FC_1 = FC(2346, 10, SimpleInitializer(0.1), SGD(0.1))\n",
    "A_FC_1 = FC_1.forward(A_flat)\n",
    "softmax = Softmax()\n",
    "A_soft = softmax.forward(A_FC_1)\n",
    "A_delta = softmax.backward(A_soft, t_train_one_hot)\n",
    "delta_Z = FC_1.backward(A_delta)\n",
    "delta_Z_reshape = delta_Z.reshape(A_relu.shape)\n",
    "delta_Z_relu = relu.backward(delta_Z_reshape)\n",
    "dZ = cnn_mnist.backward(delta_Z_relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qfwsNnY8Gx4g"
   },
   "outputs": [],
   "source": [
    "X_test = X_test.reshape(-1, 1, 784)\n",
    "t_A = cnn_mnist.forward(X_test)\n",
    "t_A = relu.forward(t_A)\n",
    "t_A  = t_A.reshape(t_A.shape[0], -1)\n",
    "t_A = FC_1.forward(t_A)\n",
    "C = np.max(t_A, axis=1)\n",
    "for i in range(t_A.shape[0]):\n",
    "    t_A[i] = np.exp(t_A[i] - C[i])\n",
    "t_A = softmax.forward(t_A)\n",
    "y = np.argmax(t_A, axis=1)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(t_test, y))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Sprint 11 - SimpleConv1d.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
