{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# solution 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "46Ptm0MiOuni"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, BatchNormalization, Activation, GlobalAveragePooling1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMlNMUJjM0Ww"
   },
   "source": [
    "# Sharing and executing the official tutorial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "mwGrCNk9PF5N",
    "outputId": "d3d0b9c5-0193-450f-8ec6-7451b8bd344e"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3193a31451b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Movie review dataset loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'keras' has no attribute 'datasets'"
     ]
    }
   ],
   "source": [
    "# Movie review dataset loading\n",
    "imdb = keras.datasets.imdb\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SBTCI9tqPIN-"
   },
   "outputs": [],
   "source": [
    "train_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vq8VvndvPZcr"
   },
   "outputs": [],
   "source": [
    "# The first part of the index is reserved\n",
    "word_index = {k:(v+3) for k,v in word_index.items()} \n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "# Create a reverse dictionary\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "# Create a function for reverse lookup\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QB46e32GPf1Q"
   },
   "outputs": [],
   "source": [
    "decode_review(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K_s3dv3OPjtr"
   },
   "outputs": [],
   "source": [
    "print(len(train_data[0]))\n",
    "print(len(train_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Obds4RXxPmle"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "train_data = sequence.pad_sequences(train_data,\n",
    "                                    value=word_index[\"<PAD>\"],\n",
    "                                    padding=\"post\",\n",
    "                                    maxlen=256)\n",
    "\n",
    "test_data = sequence.pad_sequences(test_data,\n",
    "                                    value=word_index[\"<PAD>\"],\n",
    "                                    padding=\"post\",\n",
    "                                    maxlen=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mydGXVnEPo7O"
   },
   "outputs": [],
   "source": [
    "print(len(train_data[0]))\n",
    "print(len(train_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPayBzfpPrM9"
   },
   "outputs": [],
   "source": [
    "embedding_dim=16\n",
    "vocab_size = 10000\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c107QVeePuEt"
   },
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7Nn-UiRPwhI"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "callbacks = EarlyStopping(patience=3)\n",
    "history = model.fit(train_data, train_labels, batch_size=512 ,epochs=100, callbacks=callbacks ,validation_split=0.2)\n",
    "model.evaluate(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_XZHp2oPzkX"
   },
   "outputs": [],
   "source": [
    "acc = history.history[\"accuracy\"]\n",
    "val_acc = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(12,4))\n",
    "\n",
    "axes[0].plot(epochs, loss, 'bo', label='Training loss')\n",
    "axes[0].plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "axes[0].set_title('Training and validation loss')\n",
    "axes[0].set_xlabel('Epochs')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(epochs, acc, 'bo', label='Training acc')\n",
    "axes[1].plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "axes[1].set_title('Training and validation accuracy')\n",
    "axes[1].set_xlabel('Epochs')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend(loc='lower right')\n",
    "axes[1].set_ylim((0.5,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3Bwuaj4P3At"
   },
   "outputs": [],
   "source": [
    "# Get the weight of the embedding layer\n",
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape) ## shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aayvX8peP9SM"
   },
   "source": [
    "#[Problem 2] (Advance assignment) Execute various methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_HAuT4qQBua"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import special_ortho_group\n",
    "from scipy.spatial.transform import Rotation\n",
    "from scipy.linalg import svd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "FIGURE_SCALE = 1.0\n",
    "FONT_SIZE = 20\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': np.array((8, 6)) * FIGURE_SCALE,\n",
    "    'axes.labelsize': FONT_SIZE,\n",
    "    'axes.titlesize': FONT_SIZE,\n",
    "    'xtick.labelsize': FONT_SIZE,\n",
    "    'ytick.labelsize': FONT_SIZE,\n",
    "    'legend.fontsize': FONT_SIZE,\n",
    "    'lines.linewidth': 3,\n",
    "    'lines.markersize': 10,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nrqa6vu5QES4"
   },
   "outputs": [],
   "source": [
    "def SO3_via_svd(A):\n",
    "  \"\"\"Map 3x3 matrix onto SO(3) via SVD.\"\"\"\n",
    "  u, s, vt = np.linalg.svd(A)\n",
    "  s_SO3 = [1, 1, np.sign(np.linalg.det(np.matmul(u, vt)))]\n",
    "  return np.matmul(np.matmul(u, np.diag(s_SO3)), vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gWUSl-XvQGff"
   },
   "outputs": [],
   "source": [
    "def SO3_via_gramschmidt(A):\n",
    "  \"\"\"Map 3x3 matrix on SO(3) via GS, ignores last column.\"\"\"\n",
    "  x_normalized = A[:, 0] / np.linalg.norm(A[:, 0])\n",
    "  z = np.cross(x_normalized, A[:, 1])\n",
    "  z_normalized = z / np.linalg.norm(z)\n",
    "  y_normalized = np.cross(z_normalized, x_normalized)\n",
    "  return np.stack([x_normalized, y_normalized, z_normalized], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ruB4LBKlQLZS"
   },
   "outputs": [],
   "source": [
    "def rotate_from_z(v):\n",
    "  \"\"\"Construct a rotation matrix R such that R * [0,0,||v||]^T = v.\n",
    "\n",
    "  Input v is shape (3,), output shape is 3x3 \"\"\"\n",
    "  vn = v / np.linalg.norm(v)\n",
    "  theta = np.arccos(vn[2])\n",
    "  phi = np.arctan2(vn[1], vn[0])\n",
    "  r = Rotation.from_euler('zyz', [0, theta, phi])\n",
    "  R = np.squeeze(r.as_dcm()) # Maps Z to vn\n",
    "  return R\n",
    "\n",
    "def perturb_rotation_matrix(R, kappa):\n",
    "  \"\"\"Perturb a random rotation matrix with noise.\n",
    "\n",
    "  Noise is random small rotation applied to each of the three\n",
    "  column vectors of R. Angle of rotation is sampled from the\n",
    "  von-Mises distribution on the circle (with uniform random azimuth).\n",
    "\n",
    "  The von-Mises distribution is analagous to Gaussian distribution on the circle.\n",
    "  Note, the concentration parameter kappa is inversely related to variance,\n",
    "  so higher kappa means less variance, less noise applied. Good ranges for\n",
    "  kappa are 64 (high noise) up to 512 (low noise).\n",
    "  \"\"\"\n",
    "  R_perturb = []\n",
    "  theta = np.random.vonmises(mu=0.0, kappa=kappa, size=(3,))\n",
    "  phi = np.random.uniform(low=0.0, high=np.pi*2.0, size=(3,))\n",
    "  for i in range(3):\n",
    "    v = R[:, i]\n",
    "    R_z_to_v = rotate_from_z(v)\n",
    "    r_noise_z = np.squeeze(Rotation.from_euler('zyz', [0, theta[i], phi[i]]).as_dcm())\n",
    "\n",
    "    v_perturb = np.matmul(R_z_to_v, np.matmul(r_noise_z, np.array([0,0,1])))\n",
    "    R_perturb.append(v_perturb)\n",
    "\n",
    "  R_perturb = np.stack(R_perturb, axis=-1)\n",
    "  return R_perturb\n",
    "\n",
    "\n",
    "def sigma_to_kappa(sigma):\n",
    "  return ((0.5 - sigma) * 1024) + 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XHk-XdzmQeUD"
   },
   "outputs": [],
   "source": [
    "# We create a ground truth special orthogonal matrix and perturb it with\n",
    "# additive noise. We then see which orthogonalization process (SVD or GS) is\n",
    "# better at recovering the ground truth matrix.\n",
    "\n",
    "\n",
    "def run_expt(sigmas, num_trials, noise_type='gaussian'):\n",
    "  # Always use identity as ground truth, or pick random matrix.\n",
    "  # Nothing should change if we pick random (can verify by setting to True) since\n",
    "  # SVD and Gram-Schmidt are both Equivariant to rotations.\n",
    "  pick_random_ground_truth=False\n",
    "\n",
    "  all_errs_svd = []\n",
    "  all_errs_gs = []\n",
    "  all_geo_errs_svd = []\n",
    "  all_geo_errs_gs = []\n",
    "  all_noise_norms = []\n",
    "  all_noise_sq_norms = []\n",
    "\n",
    "  for sig in sigmas:\n",
    "    svd_errors = np.zeros(num_trials)\n",
    "    gs_errors = np.zeros(num_trials)\n",
    "    svd_geo_errors = np.zeros(num_trials)\n",
    "    gs_geo_errors = np.zeros(num_trials)\n",
    "    noise_norms = np.zeros(num_trials)\n",
    "    noise_sq_norms = np.zeros(num_trials)\n",
    "\n",
    "    for t in range(num_trials):\n",
    "      if pick_random_ground_truth:\n",
    "        A = special_ortho_group.rvs(3)  # Pick a random ground truth matrix\n",
    "      else:\n",
    "        A = np.eye(3)  # Our ground truth matrix in SO(3)\n",
    "\n",
    "      N = None\n",
    "      if noise_type == 'gaussian':\n",
    "        N = np.random.standard_normal(size=(3,3)) * sig\n",
    "      if noise_type == 'uniform':\n",
    "        N = np.random.uniform(-1, 1, (3, 3)) * sig\n",
    "      if noise_type == 'rademacher':\n",
    "        N = np.sign(np.random.uniform(-1, 1, (3, 3))) * sig\n",
    "      if noise_type == 'rotation':\n",
    "        A_perturb = perturb_rotation_matrix(A, kappa=sigma_to_kappa(sig))\n",
    "        N = A_perturb - A\n",
    "      if N is None:\n",
    "        print ('Error: unknown noise_type: %s', noise_type)\n",
    "        return\n",
    "\n",
    "      AplusN = A + N  # Ground-truth plus noise\n",
    "      noise_norm = np.linalg.norm(N)\n",
    "      noise_norm_sq = noise_norm**2\n",
    "\n",
    "      # Compute SVD result and error.\n",
    "      res_svd = SO3_via_svd(AplusN)\n",
    "      error_svd = np.linalg.norm(res_svd - A, ord='fro')**2\n",
    "      error_geodesic_svd = np.arccos(\n",
    "          (np.trace(np.matmul(np.transpose(res_svd), A))-1.0)/2.0);\n",
    "\n",
    "      # Compute GS result and error.\n",
    "      res_gs = SO3_via_gramschmidt(AplusN)\n",
    "      error_gs = np.linalg.norm(res_gs - A, ord='fro')**2\n",
    "      error_geodesic_gs = np.arccos(\n",
    "          (np.trace(np.matmul(np.transpose(res_gs), A))-1.0)/2.0);\n",
    "\n",
    "      svd_errors[t] = error_svd\n",
    "      gs_errors[t] = error_gs\n",
    "      svd_geo_errors[t] = error_geodesic_svd\n",
    "      gs_geo_errors[t] = error_geodesic_gs\n",
    "      noise_norms[t] = noise_norm\n",
    "      noise_sq_norms[t] = noise_norm_sq\n",
    "\n",
    "    all_errs_svd.append(svd_errors)\n",
    "    all_errs_gs.append(gs_errors)\n",
    "    all_geo_errs_svd.append(svd_geo_errors)\n",
    "    all_geo_errs_gs.append(gs_geo_errors)\n",
    "    all_noise_norms.append(noise_norms)\n",
    "    all_noise_sq_norms.append(noise_sq_norms)\n",
    "    print('finished sigma = %f / kappa = %f' % (sig, sigma_to_kappa(sig)))\n",
    "\n",
    "  return [np.array(x) for x in (\n",
    "      all_errs_svd, all_errs_gs,\n",
    "      all_geo_errs_svd, all_geo_errs_gs,\n",
    "      all_noise_norms, all_noise_sq_norms)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHG2xT4mQhMx"
   },
   "outputs": [],
   "source": [
    "boxprops = dict(linewidth=2)\n",
    "medianprops = dict(linewidth=2)\n",
    "whiskerprops = dict(linewidth=2)\n",
    "capprops = dict(linewidth=2)\n",
    "\n",
    "def make_diff_plot(svd_errs, gs_errs, xvalues, title='', ytitle='', xtitle=''):\n",
    "  plt.figure(figsize=(8,6))\n",
    "  plt.title(title, fontsize=16)\n",
    "  diff = gs_errs - svd_errs\n",
    "  step_size = np.abs(xvalues[1] - xvalues[0])\n",
    "  plt.boxplot(diff.T, positions=xvalues, widths=step_size/2, whis=[5, 95],\n",
    "              boxprops=boxprops, medianprops=medianprops, whiskerprops=whiskerprops, capprops=capprops,\n",
    "              showmeans=False, meanline=True, showfliers=False)\n",
    "  plt.plot(xvalues, np.max(diff, axis=1), 'kx', markeredgewidth=2)\n",
    "  plt.plot(xvalues, np.min(diff, axis=1), 'kx', markeredgewidth=2)\n",
    "  xlim = [np.min(xvalues) - (step_size / 3), np.max(xvalues) + (step_size / 3)]\n",
    "  plt.xlim(xlim)\n",
    "  plt.plot(xlim, [0, 0], 'k--', linewidth=1)\n",
    "  plt.xlabel(xtitle, fontsize=16)\n",
    "  plt.ylabel(ytitle, fontsize=16)\n",
    "  plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79JmhnbVQlPr"
   },
   "source": [
    "###Global Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UX6ak3_7QrE0"
   },
   "outputs": [],
   "source": [
    "num_trials = 100000  # Num trials at each sigma\n",
    "sigmas = np.linspace(0.125, 0.5, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvp6Lcl2QtRc"
   },
   "source": [
    "###Gaussian Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yrmGH3IRQxHe"
   },
   "outputs": [],
   "source": [
    "(all_errs_svd, all_errs_gs,\n",
    " all_geo_errs_svd, all_geo_errs_gs,\n",
    " all_noise_norms, all_noise_sq_norms\n",
    " ) = run_expt(sigmas, num_trials, noise_type='gaussian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJ3zl9q8Q0La"
   },
   "outputs": [],
   "source": [
    "plt.plot(sigmas,\n",
    "         3*sigmas**2,\n",
    "         '--b',\n",
    "         label='3 $\\\\sigma^2$')\n",
    "plt.errorbar(sigmas,\n",
    "             all_errs_svd.mean(axis=1),\n",
    "             color='b',\n",
    "             label='E[$\\\\|\\\\|\\\\mathrm{SVD}^+(M) - R\\\\|\\\\|_F^2]$')\n",
    "\n",
    "plt.plot(sigmas, 6*sigmas**2,\n",
    "         '--r',\n",
    "         label='6 $\\\\sigma^2$')\n",
    "plt.errorbar(sigmas,\n",
    "             all_errs_gs.mean(axis=1),\n",
    "             color='r',\n",
    "             label='E[$\\\\|\\\\|\\\\mathrm{GS}^+(M) - R\\\\|\\\\|_F^2$]')\n",
    "\n",
    "plt.xlabel('$\\\\sigma$')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1EJuqO9Q4R4"
   },
   "outputs": [],
   "source": [
    "make_diff_plot(all_errs_svd, all_errs_gs, sigmas, title='Gaussian Noise', ytitle='Frobenius Error Diff', xtitle='$\\\\sigma$')\n",
    "make_diff_plot(all_geo_errs_svd, all_geo_errs_gs, sigmas, title='Gaussian Noise', ytitle='Geodesic Error Diff', xtitle='$\\\\sigma$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-RCtzWMMRD67"
   },
   "outputs": [],
   "source": [
    "(all_errs_svd, all_errs_gs,\n",
    " all_geo_errs_svd, all_geo_errs_gs,\n",
    " all_noise_norms, all_noise_sq_norms\n",
    " ) = run_expt(sigmas, num_trials, noise_type='uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QYZbjkVMRGhF"
   },
   "outputs": [],
   "source": [
    "make_diff_plot(all_errs_svd, all_errs_gs, sigmas, title='Uniform Noise', ytitle='Frobenius Error Diff', xtitle='$\\\\phi$')\n",
    "make_diff_plot(all_geo_errs_svd, all_geo_errs_gs, sigmas, title='Uniform Noise', ytitle='Geodesic Error Diff', xtitle='$\\\\phi$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UyhpzFYBRTB9"
   },
   "source": [
    "# solution 3 Learning Iris (binary classification) with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "734G1XXCRWWi"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[50:]\n",
    "y = iris.target[50:]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5kWHhw5RYjb"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "sc = StandardScaler().fit(X_train)\n",
    "X_train = sc.transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "le = LabelBinarizer()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)\n",
    "y_test = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YoQeiZbcRb8k"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "def iris_nn_classifier(n_features):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(64, activation=\"relu\", input_shape=(n_features,)))\n",
    "  model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "  model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "  return model\n",
    "\n",
    "n_features = 4\n",
    "model = iris_nn_classifier(n_features)\n",
    "history = model.fit(X_train, y_train, batch_size=20, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dh8vAOuJRqXZ"
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kGQH36UnRz9o"
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "sc = StandardScaler().fit(X_train)\n",
    "X_train = sc.transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZ36T2uTR37m"
   },
   "outputs": [],
   "source": [
    "def iris_nn_multi_classifier(n_features):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(256, activation=\"relu\", input_shape=(n_features,)))\n",
    "  model.add(Dense(128, activation=\"relu\"))\n",
    "  model.add(Dense(64, activation=\"relu\"))\n",
    "  model.add(Dense(3, activation=\"softmax\"))\n",
    "\n",
    "  model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "  return model\n",
    "\n",
    "n_features = 4\n",
    "model = iris_nn_multi_classifier(n_features)\n",
    "history = model.fit(X_train, y_train, batch_size=20, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bp_7EQVbR7x7"
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjJWIBQWR-0a"
   },
   "source": [
    "# Learning House Prices with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHiU7UHrSCDs"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "X_train = df.loc[:, [\"TotalBsmtSF\", \"YearBuilt\", \"GarageArea\"]]\n",
    "y_train = df.loc[:, \"SalePrice\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, random_state=0)\n",
    "\n",
    "sc = StandardScaler().fit(X_train)\n",
    "X_train = sc.transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uwEa2FrlSFbz"
   },
   "outputs": [],
   "source": [
    "def nn_regression(n_features, n_output):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(256, activation=\"relu\", input_shape=(n_features,)))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Dense(128, activation=\"relu\"))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Dense(64, activation=\"relu\"))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Dense(n_output, activation=\"linear\"))\n",
    "\n",
    "  model.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"mse\"])\n",
    "  return model\n",
    "\n",
    "callbacks = EarlyStopping(patience=3)\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "n_output = 1\n",
    "model = nn_regression(n_features, n_output)\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=10, callbacks=callbacks, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AHSOj0CQSIiP"
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZAnwTx4SNN-"
   },
   "source": [
    "# Learning MNIST with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uoXFiTfoSQyL"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fMaaf-fWSTYM"
   },
   "outputs": [],
   "source": [
    "img_height = 28\n",
    "img_width = 28\n",
    "num_features = int(img_height * img_width)\n",
    "\n",
    "X_train = X_train.reshape(-1, num_features).astype(\"float\")\n",
    "X_test = X_test.reshape(-1, num_features).astype(\"float\")\n",
    "\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "y_train = to_categorical(y_train.reshape(-1,1))\n",
    "y_test = to_categorical(y_test.reshape(-1,1))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sEPBrjFjSWU6"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def mnist_classifier(n_features, n_output):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(256, activation=\"relu\", input_shape=(n_features,)))\n",
    "  model.add(Dense(128, activation=\"relu\"))\n",
    "  model.add(Dense(64, activation=\"relu\"))\n",
    "  model.add(Dense(n_output, activation=\"softmax\"))\n",
    "\n",
    "  model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "  return model\n",
    "\n",
    "callbacks = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=2)\n",
    "\n",
    "model = mnist_classifier(784, 10)\n",
    "history = model.fit(X_train, y_train, batch_size=20, epochs=10, callbacks=callbacks, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1WSCSjZNSZD2"
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4DvdC-TSgS8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "muiL-mUXSirW"
   },
   "outputs": [],
   "source": [
    "#Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "#Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fcTvpNIuSl0W"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dbnmJ-ruSstJ"
   },
   "outputs": [],
   "source": [
    "#Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "#Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQpByZchSzSu"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5WGcgLlGTFUX"
   },
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MUua4PJNTIgt"
   },
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HfGbHX31TLRR"
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AVOZZ8XPTPUC"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVfaLbRVuZLo"
   },
   "source": [
    "###PyTorch Convolutional Neural Network With MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MD4DTD4iudsN"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8-XIhSwugYB"
   },
   "outputs": [],
   "source": [
    "#Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hz0VwWofunLF"
   },
   "outputs": [],
   "source": [
    "#Download MNIST dataset in local system\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "train_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = True,                         \n",
    "    transform = ToTensor(), \n",
    "    download = True,            \n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    root = 'data', \n",
    "    train = False, \n",
    "    transform = ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehflcWAYuuQP"
   },
   "outputs": [],
   "source": [
    "#Print train_data and test_data size\n",
    "print(train_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gH4xhI85ux6T"
   },
   "outputs": [],
   "source": [
    "print(train_data.data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oxmy8Yyqu0Zk"
   },
   "outputs": [],
   "source": [
    "#Visualization of MNIST dataset\n",
    "#Plot one train_data\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(train_data.data[0], cmap='gray')\n",
    "plt.title('%i' % train_data.targets[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50y3gi6mu5A2"
   },
   "outputs": [],
   "source": [
    "#Plot multiple train_data\n",
    "figure = plt.figure(figsize=(10, 8))\n",
    "cols, rows = 5, 5\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(train_data), size=(1,)).item()\n",
    "    img, label = train_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(label)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYJwkCDju82p"
   },
   "outputs": [],
   "source": [
    "#Preparing data for training with DataLoaders\n",
    "from torch.utils.data import DataLoader\n",
    "loaders = {\n",
    "    'train' : torch.utils.data.DataLoader(train_data, \n",
    "                                          batch_size=100, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1),\n",
    "    \n",
    "    'test'  : torch.utils.data.DataLoader(test_data, \n",
    "                                          batch_size=100, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1),\n",
    "}\n",
    "loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VSO52KJbvIh9"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(         \n",
    "            nn.Conv2d(\n",
    "                in_channels=1,              \n",
    "                out_channels=16,            \n",
    "                kernel_size=5,              \n",
    "                stride=1,                   \n",
    "                padding=2,                  \n",
    "            ),                              \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(kernel_size=2),    \n",
    "        )\n",
    "        self.conv2 = nn.Sequential(         \n",
    "            nn.Conv2d(16, 32, 5, 1, 2),     \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(2),                \n",
    "        )\n",
    "        # fully connected layer, output 10 classes\n",
    "        self.out = nn.Linear(32 * 7 * 7, 10)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        x = x.view(x.size(0), -1)       \n",
    "        output = self.out(x)\n",
    "        return output, x    # return x for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B59JMHOxvLcO"
   },
   "outputs": [],
   "source": [
    "cnn = CNN()\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NSetsCEXvOeh"
   },
   "outputs": [],
   "source": [
    "#Define loss function\n",
    "loss_func = nn.CrossEntropyLoss()   \n",
    "loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6R15qKcvSPA"
   },
   "outputs": [],
   "source": [
    "#Define a Optimization Function\n",
    "from torch import optim\n",
    "optimizer = optim.Adam(cnn.parameters(), lr = 0.01)   \n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6SEsakBFvXIx"
   },
   "outputs": [],
   "source": [
    "#Train the model\n",
    "from torch.autograd import Variable\n",
    "num_epochs = 10\n",
    "def train(num_epochs, cnn, loaders):\n",
    "    \n",
    "    cnn.train()\n",
    "        \n",
    "    #Train the model\n",
    "    total_step = len(loaders['train'])\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            \n",
    "            #gives batch data, normalize x when iterate train_loader\n",
    "            b_x = Variable(images)   # batch x\n",
    "            b_y = Variable(labels)   # batch y\n",
    "output = cnn(b_x)[0]               \n",
    "            loss = loss_func(output, b_y)\n",
    "            \n",
    "            #clear gradients for this training step   \n",
    "            optimizer.zero_grad()           \n",
    "            \n",
    "            #backpropagation, compute gradients \n",
    "            loss.backward()    \n",
    "            #apply gradients             \n",
    "            optimizer.step()                \n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                       .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
    "               pass\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    \n",
    "    pass\n",
    "train(num_epochs, cnn, loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8halLjNAvgTz"
   },
   "outputs": [],
   "source": [
    "#Evaluate the model on test data\n",
    "def test():\n",
    "    # Test the model\n",
    "    cnn.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in loaders['test']:\n",
    "            test_output, last_layer = cnn(images)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "            accuracy = (pred_y == labels).sum().item() / float(labels.size(0))\n",
    "            pass\n",
    "print('Test Accuracy of the model on the 10000 test images: %.2f' % accuracy)\n",
    "    \n",
    "    pass\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ai59m-8mvj9G"
   },
   "outputs": [],
   "source": [
    "sample = next(iter(loaders['test']))\n",
    "imgs, lbls = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kGHPakvAvmPH"
   },
   "outputs": [],
   "source": [
    "actual_number = lbls[:10].numpy()\n",
    "actual_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_2IgVB3bvo9h"
   },
   "outputs": [],
   "source": [
    "test_output, last_layer = cnn(imgs[:10])\n",
    "pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "print(f'Prediction number: {pred_y}')\n",
    "print(f'Actual number: {actual_number}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rTlJYr0vt7x"
   },
   "source": [
    "# Predict house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8SSbDflgvx-Z"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxAgqSdyv2bZ"
   },
   "outputs": [],
   "source": [
    "pd.read_csv('california_housing_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "An_ITosQv5WR"
   },
   "outputs": [],
   "source": [
    "#getting std and mean of training data, as it needs for normalization and denormalization of data\n",
    "\n",
    "train_csv = pd.read_csv('california_housing_train.csv') \n",
    "train_mean =  train_csv.mean()\n",
    "train_std = train_csv.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9GyKesIv80I"
   },
   "outputs": [],
   "source": [
    "#creating custom dataset class\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.norm_data = (self.data_frame - train_mean)/train_std  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.norm_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.norm_data.iloc[idx, 2:8].values # keep all except median_house_value  \n",
    "        label = self.norm_data.iloc[idx, 8:9].values # keep only median_house_value  \n",
    "        \n",
    "        data = torch.tensor(data, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "        return {'data': data, 'label':label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtr9jcagwBN4"
   },
   "outputs": [],
   "source": [
    "#Loading our dataset object in DataLoader class\n",
    "train_data = MyDataset('california_housing_train.csv')\n",
    "dataset_len = len(train_data)\n",
    "train_data = torch.utils.data.DataLoader(dataset=train_data, shuffle=True, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-EpIPjywEUj"
   },
   "outputs": [],
   "source": [
    "#Create artificial neural network model class\n",
    "#our model\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=6, out_features=18, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=18, out_features=18, bias=True)\n",
    "        self.fc3 = nn.Linear(in_features=18, out_features=12, bias=True)\n",
    "        self.fc4 = nn.Linear(in_features=12, out_features=1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "net = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rKPm7Kd4wJNK"
   },
   "outputs": [],
   "source": [
    "#Define optimization and loss functions\n",
    "#loss and optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_CLnu28AwMgV"
   },
   "outputs": [],
   "source": [
    "#Create training loop\n",
    "#training the model\n",
    "\n",
    "for epoch in range(15):\n",
    "    running_loss = 0.0\n",
    "    for i, value in enumerate(train_data):\n",
    "        inputs = value['data']\n",
    "        labels = value['label']\n",
    "        prediction = net(inputs) # passing inputs to our model to get prediction\n",
    "        loss = criterion(prediction, labels)\n",
    "        running_loss += loss.item() * inputs.size(0) # multiplying with batch size\n",
    "        optimizer.zero_grad() # reset all gradient calculation\n",
    "        loss.backward() # this is backpropagation to calculate gradients\n",
    "        optimizer.step() # applying gradient descent to update weights and bias values\n",
    "\n",
    "    print('epoch: ', epoch, ' loss: ', running_loss/dataset_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2SSmw8mwQlL"
   },
   "outputs": [],
   "source": [
    "#test the model\n",
    "\n",
    "test_data = MyDataset('california_housing_test.csv')\n",
    "test_data_len = len(test_data)\n",
    "test_dataset = torch.utils.data.DataLoader(dataset=test_data, shuffle=False, batch_size=10)\n",
    "\n",
    "running_loss = 0.0\n",
    "accuracy = 0.0\n",
    "for i, value in enumerate(test_dataset):\n",
    "    inputs = value['data']\n",
    "    labels = value['label']\n",
    "\n",
    "    prediction = net(inputs)\n",
    "    loss = criterion(prediction, labels)\n",
    "\n",
    "        \n",
    "    running_loss += loss.item() * inputs.size(0) # multiplying by batch size\n",
    "\n",
    "print('Test loss: ', running_loss/test_data_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VdzhzOtxwT8W"
   },
   "outputs": [],
   "source": [
    "#let's see first 10 predictions\n",
    "\n",
    "test_dataset_sample = next(iter(test_dataset)) # return first batch of data\n",
    "outputs = net(test_dataset_sample['data']) \n",
    "outputs = (outputs * train_std.values[-1]) + train_mean.values[-1] # denormalizing data to see real prices\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0MpJsgTwXvK"
   },
   "outputs": [],
   "source": [
    "pd.read_csv('california_housing_test.csv').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGFyaYd0wa6h"
   },
   "source": [
    "# Classifying the Iris Data Set with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZXyvOhPzwk-B"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJksBaYwwnxt"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris['data']\n",
    "y = iris['target']\n",
    "names = iris['target_names']\n",
    "feature_names = iris['feature_names']\n",
    "\n",
    "# Scale data to have mean 0 and variance 1 \n",
    "# which is importance for convergence of the neural network\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data set into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEI2vTApwqqY"
   },
   "outputs": [],
   "source": [
    "## Visualize the Data\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "for target, target_name in enumerate(names):\n",
    "    X_plot = X[y == target]\n",
    "    ax1.plot(X_plot[:, 0], X_plot[:, 1], \n",
    "             linestyle='none', \n",
    "             marker='o', \n",
    "             label=target_name)\n",
    "ax1.set_xlabel(feature_names[0])\n",
    "ax1.set_ylabel(feature_names[1])\n",
    "ax1.axis('equal')\n",
    "ax1.legend();\n",
    "\n",
    "for target, target_name in enumerate(names):\n",
    "    X_plot = X[y == target]\n",
    "    ax2.plot(X_plot[:, 2], X_plot[:, 3], \n",
    "             linestyle='none', \n",
    "             marker='o', \n",
    "             label=target_name)\n",
    "ax2.set_xlabel(feature_names[2])\n",
    "ax2.set_ylabel(feature_names[3])\n",
    "ax2.axis('equal')\n",
    "ax2.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jG22frUiwuNc"
   },
   "outputs": [],
   "source": [
    "## Configure Neural Network Models\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YFj1d_a6ww4W"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Model, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 50)\n",
    "        self.layer2 = nn.Linear(50, 50)\n",
    "        self.layer3 = nn.Linear(50, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.softmax(self.layer3(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKaUfLvQwzcu"
   },
   "outputs": [],
   "source": [
    "model     = Model(X_train.shape[1])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn   = nn.CrossEntropyLoss()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HM8yyVvrw2Jn"
   },
   "outputs": [],
   "source": [
    "## Train the Model\n",
    "import tqdm\n",
    "\n",
    "EPOCHS  = 100\n",
    "X_train = Variable(torch.from_numpy(X_train)).float()\n",
    "y_train = Variable(torch.from_numpy(y_train)).long()\n",
    "X_test  = Variable(torch.from_numpy(X_test)).float()\n",
    "y_test  = Variable(torch.from_numpy(y_test)).long()\n",
    "\n",
    "loss_list     = np.zeros((EPOCHS,))\n",
    "accuracy_list = np.zeros((EPOCHS,))\n",
    "\n",
    "for epoch in tqdm.trange(EPOCHS):\n",
    "    y_pred = model(X_train)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    loss_list[epoch] = loss.item()\n",
    "    \n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test)\n",
    "        correct = (torch.argmax(y_pred, dim=1) == y_test).type(torch.FloatTensor)\n",
    "        accuracy_list[epoch] = correct.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qdC0amLw6yj"
   },
   "outputs": [],
   "source": [
    "## Plot Accuracy and Loss from Training\n",
    "fig, (ax1, ax2) = plt.subplots(2, figsize=(12, 6), sharex=True)\n",
    "\n",
    "ax1.plot(accuracy_list)\n",
    "ax1.set_ylabel(\"validation accuracy\")\n",
    "ax2.plot(loss_list)\n",
    "ax2.set_ylabel(\"validation loss\")\n",
    "ax2.set_xlabel(\"epochs\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDY-OIr4w-IP"
   },
   "outputs": [],
   "source": [
    "## Show ROC Curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "# One hot encoding\n",
    "enc = OneHotEncoder()\n",
    "Y_onehot = enc.fit_transform(y_test[:, np.newaxis]).toarray()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test).numpy()\n",
    "    fpr, tpr, threshold = roc_curve(Y_onehot.ravel(), y_pred.ravel())\n",
    "    \n",
    "plt.plot(fpr, tpr, label='AUC = {:.3f}'.format(auc(fpr, tpr)))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Sprint 14 - Keras1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
